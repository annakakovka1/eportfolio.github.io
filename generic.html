Anna Kakovka
Menu
Home
Personal Statement
Deciphering Big Data
Machine Learning
Research Methods and Professional Practice
Deciphering Big Data
In a world overflowing with information, Big Data holds the key to understanding patterns that drive innovation, decision-making, and growth. The true challenge lies not in collecting data, but in deciphering it — transforming vast, unstructured information into meaningful insights. My journey in exploring Big Data focuses on bridging the gap between complexity and clarity, leveraging analytics, visualisation, and critical thinking to reveal stories hidden within the numbers.

Learning Outcomes
I will learn:

Show a comprehensive understanding of the core principles and signature features of Big Data as they manifest in Data Science.
Use a combination of data collection, cleaning, and preprocessing methods to transform messy datasets into a truly usable format.
Deploy analytical and machine-learning approaches like a comb across the data, lifting out the faint, almost hidden patterns and the gems of insight they hold.
Create data visualisations that clearly present complex findings with precision.
Evaluate issues surrounding data ethics, privacy considerations and governance structures that pop up across the sprawling landscape of big-data environments.
Interlace findings with data-driven solutions, giving decision-makers a solid foundation for informed choices across a kaleidoscope of contexts.
Discussion
Despite these benefits, Huxley et al. (2020) remind us that large-scale data collection comes with serious challenges. IoT devices generate massive amounts of data in different formats, which often makes it hard to clean, integrate and interpret effectively (Mansouri et al., 2021). Security and privacy risks are also significant — billions of connected devices increase the chances of data breaches and misuse of personal information (Tawalbeh et al., 2020). To manage these risks, frameworks such as the NIST IoT Security Baseline (Fagan et al., 2020) and the UK ICO’s IoT guidance (ICO, 2025) stress the importance of encryption, privacy-by-design, and limiting unnecessary data collection.

Overall, while IoT offers great opportunities for innovation and smarter systems, it must be developed responsibly. Balancing the benefits of data-driven insights with strong governance and ethical oversight is key to making IoT a trustworthy and sustainable technology.

References
Fagan, M. et al. (2020) NISTIR 8259A: IoT Device Cybersecurity Capability Core Baseline. Gaithersburg, MD: NIST.

Huxley, [Initials]. et al. (2020) [Title of article]. [Journal/Publisher].

ICO (2025) Guidance for consumer Internet of Things products and services. Information Commissioner’s Office.

Mansouri, T., Farid, F. and Mammadli, A. (2021) ‘IoT Data Quality Issues and Potential Solutions’, arXiv preprint, arXiv:2103.13303.

Nasajpour, M. et al. (2020) ‘Internet of Things for Current COVID-19 and Future Pandemics’, IEEE Access, 8, pp. 188802–188826.

Nižetić, S. et al. (2020) ‘Internet of Things (IoT): Opportunities, issues and challenges’, Journal of Cleaner Production, 274, 122877.

Tawalbeh, L. et al. (2020) ‘IoT Privacy and Security: Challenges and Solutions’, Applied Sciences, 10(12), 4102.

Activities
Web Scraping “Data Scientist” Jobs with Python

This Python script demonstrates how to use Requests and BeautifulSoup4 to scrape job postings containing the keyword “Data Scientist”. It fetches data from a tutorial-safe website (Fake Jobs Demo Page) and saves it as a JSON file.


# scrape_data_scientist_jobs.py
"""
Scrape job postings containing 'Data Scientist' and save them to a JSON file.
Uses Requests for HTTP and BeautifulSoup for parsing HTML.
"""

import requests
from bs4 import BeautifulSoup
import json
import time

# 1. Identify the webpage (safe demo site)
URL = "https://realpython.github.io/fake-jobs/"
KEYWORD = "Data Scientist"

# 2. Send HTTP request
response = requests.get(URL)
response.raise_for_status()  # ensure successful request

# 3. Parse HTML with BeautifulSoup
soup = BeautifulSoup(response.text, "html.parser")

# 4. Extract relevant data
jobs = []
for job_card in soup.find_all("div", class_="card-content"):
    title = job_card.find("h2", class_="title").get_text(strip=True)
    if KEYWORD.lower() in title.lower():
        company = job_card.find("h3", class_="company").get_text(strip=True)
        location = job_card.find("p", class_="location").get_text(strip=True)
        link = job_card.find_next("a", string="Apply")["href"]
        jobs.append({
            "title": title,
            "company": company,
            "location": location,
            "apply_url": link,
            "keyword": KEYWORD,
            "scraped_at": time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime())
        })

# 5. Store data in JSON format
output_file = "data_scientist_jobs.json"
with open(output_file, "w", encoding="utf-8") as f:
    json.dump(jobs, f, indent=2, ensure_ascii=False)

print(f"✅ Scraped {len(jobs)} '{KEYWORD}' job(s). Data saved to {output_file}.")
Note: This example uses a public tutorial site. Always check a site’s robots.txt file and terms of service before scraping.

Summary of findings

I used requests to fetch an HTML page and BeautifulSoup to parse job listings containing the keyword “Data Scientist”.
The extracted data — including title, company, location, and apply link — were saved as a JSON file.
The process followed the Request → Protocol → Response pattern of inter-process communication.
Critical reflection

The main challenge was ensuring reliability and respecting ethical scraping rules.
I used a static demo site (safe to scrape) to avoid legal or security issues.
Data wrangling challenges include cleaning inconsistent data, changing page structures, and network errors.
Using structured formats like JSON facilitates interoperability and data sharing.
In a team context, proper task separation (developer, tester, analyst) and documentation are essential for maintainability and reproducibility.
Test
Data Management Pipeline Test — Reflection & Evidence
Result summary
State: Finished
Time taken: 24 mins 17 secs
Marks: 2.00 / 2.00
Grade: 10.00 / 10.00 (100%)
Date: Sunday, 19 Oct 2025
Course: DBD_PCOM7E (July 2025)
What I demonstrated
Matched Python concepts to their purposes (in/not in, dict.values(), string formatting, CSV writer, strftime/strptime, list comprehensions).
Matched Python practices to descriptions (PEP-8 naming & imports, repository organization, unit testing, “fast but clear,” version control, documentation, “use libraries”).
Achieved a 100% overall grade across both questions.
Skills tagged
Python fundamentals • Code quality (PEP-8) • Data handling (CSV, datetime) • Software engineering practices • Version control (Git) • Documentation & testing • Problem solving

View full results
Test
What went well: I quickly recognized standard Python concepts and practices and applied them accurately. Concrete examples (e.g., strftime/strptime for dates, CSV writer for persistence, list comprehensions for fast data shaping) made each match unambiguous.

What I reinforced: Clarity over cleverness (“fast but clear”); imports should be minimal and explicit; organizing a repository (plus READMEs) is as important as writing code. Clear naming and targeted exceptions/documentation reduce debugging time.

Next steps: Strengthen testing discipline by adding unit tests to upcoming mini-projects, practice more with the datetime formatting table, and continue using/contributing to high-quality libraries instead of rewriting solutions.

Meeting Notes – Database Design Proposal: Deciphering Big Data
Date: 6th of September, 2025
Attendees: Kalifa, Daniella and Anna
Prepared by: Anna
1. Project Overview The client operates in Private Healthcare Services (UK), offering GP consultations, specialist clinics, diagnostic tests, and billing management. Objective (Updated by AK): Build a secure, logical database for managing patient records, appointments, treatments, and billing while ensuring GDPR and NHS compliance.
2. Key Additions / Changes ( AK’s Edits) - Expanded Objectives section to emphasize security, compliance, and scalability.
- Clarified data entities (Patient, Healthcare Professionals, Appointment, Treatment, Billing, Department, Insurance Provider).
- Added details on data types/formats for standardization.
- Updated attributes to include:
    • EmergencyContactName and Phone
    • NHSNumber
- Enhanced clarity on foreign key relationships.
- Proposed addition: Include an ER diagram and sample SQL queries for demonstration.
- Defined role-based access for data security.
- Expanded data management pipeline with a detailed cleansing workflow.
- Suggested section: Add a Summary/Evaluation to discuss challenges, limitations, and improvements for future phases.
3. Technical Selections - DBMS: MySQL chosen for cost-effectiveness, scalability, and ACID compliance.
- Security: Role-based access + encryption for sensitive data (e.g., NHS number, contact details).
- Operations: CRUD-based management and reporting queries (e.g., patient history, departmental workload).
4. Next Steps - Add ER diagram and sample SQL queries (AK’s note).
- Develop Summary/Evaluation section.
- Review data validation rules for accuracy and compliance.
- Schedule a follow-up review with the client for design confirmation.
Part 2 – Reflection (≈1,000 words)
Introduction
This reflection employs Rolfe et al.’s (2001) model What? So what? Now what? to analyse learning and development during the MedicaidUK database project. It focuses on technical competence, teamwork, and professional growth achieved through engagement with the data-wrangling module.

What?
The project objective was to design a secure and scalable database for MedicaidUK. My individual responsibilities included logical schema creation, validation of entity relationships, and documentation of compliance mechanisms. I contributed to the initial design proposal, developed entity attributes, and assisted in implementing schema components within MySQL.

Engagement in data-wrangling activities involved extracting datasets from simulated sources, cleaning inconsistent records, and testing validation rules. I applied techniques such as deduplication, type standardisation, and referential checks to ensure data reliability.

The collaborative aspect was equally significant. Regular virtual meetings allowed for reviewing schema consistency and resolving design conflicts. These sessions required concise technical communication and collective decision-making to achieve consensus on entity structures and naming conventions.

So What?
Working on this project clarified how abstract data-modelling theory operates within practical system design. Converting conceptual models into executable SQL schemas exposed challenges in maintaining referential integrity, enforcing constraints, and balancing normalisation with query efficiency. The iterative process revealed that even minor design inconsistencies can have cascading effects on functionality and reporting accuracy.

A deeper understanding of ethical and legal dimensions of data management also emerged. Implementing access-control layers and encryption concepts provided insight into how technical design upholds confidentiality and trust. Marquis (2024) emphasised the preventive function of role-based access control, a principle integrated into the final schema.

On a personal level, initial frustration over syntax errors and design revisions gradually transformed into appreciation for precision and structured problem-solving. The experience emphasised perseverance and collaboration as essential professional attributes. Feedback from peers and tutors promoted reflective practice and critical thinking, shifting focus from merely completing tasks to evaluating decisions in relation to best practice and research evidence.

The emotional and cognitive engagement throughout the project strengthened confidence in handling complexity and communicating rationale to non-technical audiences. It highlighted that effective database design is not solely a technical exercise but also a process of reasoning, negotiation, and accountability.

Now What?
The knowledge gained from this project provides a foundation for future professional advancement. Continued development will focus on extending expertise in cloud-based database management, data-pipeline automation, and advanced analytics. The integration of predictive modelling and machine-learning techniques, as discussed by Na et al. (2023), represents a logical progression for applying these competencies to enhance healthcare operations.

Further training in cybersecurity and data-protection frameworks will strengthen capacity to design and audit compliant systems. Participation in industry certifications related to data governance or privacy will consolidate the theoretical and practical learning achieved in this module.

From a collaborative perspective, I will apply lessons learned about team coordination and communication in future projects. Establishing clearer task allocation, documentation standards, and version control will improve efficiency and reduce conflict in distributed teams.

Summary
In summary, this reflective process has affirmed the importance of ongoing learning and ethical responsibility in data-focused professions. The MedicaidUK project turned theoretical knowledge into practical skill, aligning technical expertise with professional integrity and flexibility. The insights gained will inform future contributions to data management projects that require analytical accuracy, compliance awareness, and interdisciplinary teamwork.

References
Batko, K. and Ślęzak, A. (2022) ‘The use of big data analytics in healthcare.’ Journal of Big Data, 9(3), pp. 1–24.

Bitrián, P., Buil, I., Catalán, S. and Merli, D. (2024) ‘Gamification in workforce training: Improving employees’ self-efficacy and information security and data protection behaviours.’ Journal of Business Research, 179, p. 114685.

Marquis, Y. A. (2024) ‘From theory to practice: Implementing effective role-based access control strategies to mitigate insider risks in diverse organisational contexts.’ Journal of Engineering Research and Reports, 26(5), pp. 138–154.

Na, L. et al. (2023) ‘Patient outcome predictions improve operations at a large hospital network.’ arXiv.org. Available at: https://arxiv.org/abs/2305.15629.

Oracle (2025) MySQL Workbench [Computer Program]. Available at: https://www.mysql.com/products/workbench/ (Accessed: 6 Sept 2025).

Rolfe, G., Freshwater, D. and Jasper, M. (2001) Critical reflection in nursing and the helping professions: A user’s guide. Basingstoke: Palgrave Macmillan.

Tariq, S., Tariq, S. and Ahmad Adnan Shoukat (2023) ‘Centralised healthcare database for ensuring better healthcare: Are we lagging behind?’ Pakistan Journal of Medical Sciences, 40(3).

GitHub
LinkedIn
© Untitled
Design: HTML5 UP
